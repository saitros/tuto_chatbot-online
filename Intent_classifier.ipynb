{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # Colab only\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/juns/lib/python3.6/site-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n",
      "/home/ubuntu/anaconda3/envs/juns/lib/python3.6/site-packages/jpype/_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from konlpy.tag import Twitter\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "keras = tf.keras\n",
    "t = Twitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. fit tokenizer to our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorizer import BaseVectorizer\n",
    "tokenizer = BaseVectorizer(t.morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Intent/train_intent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>여자 영어 말해줄래요</td>\n",
       "      <td>번역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>고장 영어 뭔데요</td>\n",
       "      <td>번역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>방문 영어 뭔지 알아요</td>\n",
       "      <td>번역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>코끼리 영어 뭔지 알려줄래요</td>\n",
       "      <td>번역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>파인애플 영어 번역요</td>\n",
       "      <td>번역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>배 뭔지 영어 번역해줘요</td>\n",
       "      <td>번역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>전기 번역해주세요</td>\n",
       "      <td>번역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>물고기 영어 번역해줄래요</td>\n",
       "      <td>번역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>이 문장 영어 말해줘요</td>\n",
       "      <td>번역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>이 단어 영어 말해주세요</td>\n",
       "      <td>번역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>이 문장 영어 말해줘요</td>\n",
       "      <td>번역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>이 단어 영어 말해주세요</td>\n",
       "      <td>번역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>영어 알려줘요</td>\n",
       "      <td>번역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>존경해 중국어 뭐에요</td>\n",
       "      <td>번역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>바보 중국어 알려줘요</td>\n",
       "      <td>번역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>누나 중국어 어떻게 말해요</td>\n",
       "      <td>번역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>중국어 알려줘요</td>\n",
       "      <td>번역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>바이올린 중국어 말해줘요</td>\n",
       "      <td>번역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>중국어 밥이 뭐니요</td>\n",
       "      <td>번역</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>셔츠 중국어 뭔데요</td>\n",
       "      <td>번역</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            question intent\n",
       "500      여자 영어 말해줄래요     번역\n",
       "501        고장 영어 뭔데요     번역\n",
       "502     방문 영어 뭔지 알아요     번역\n",
       "503  코끼리 영어 뭔지 알려줄래요     번역\n",
       "504      파인애플 영어 번역요     번역\n",
       "505    배 뭔지 영어 번역해줘요     번역\n",
       "506        전기 번역해주세요     번역\n",
       "507    물고기 영어 번역해줄래요     번역\n",
       "508     이 문장 영어 말해줘요     번역\n",
       "509    이 단어 영어 말해주세요     번역\n",
       "510     이 문장 영어 말해줘요     번역\n",
       "511    이 단어 영어 말해주세요     번역\n",
       "512          영어 알려줘요     번역\n",
       "513      존경해 중국어 뭐에요     번역\n",
       "514      바보 중국어 알려줘요     번역\n",
       "515   누나 중국어 어떻게 말해요     번역\n",
       "516         중국어 알려줘요     번역\n",
       "517    바이올린 중국어 말해줘요     번역\n",
       "518       중국어 밥이 뭐니요     번역\n",
       "519       셔츠 중국어 뭔데요     번역"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[500:520]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning was done                                        \n",
      "1405 terms are recognized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<vectorizer.BaseVectorizer at 0x7f914c3b0f98>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.fit(df['question'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_PAD_': 0,\n",
       " '_UNK_': 1,\n",
       " '_STA_': 2,\n",
       " '_EOS_': 3,\n",
       " '오늘': 4,\n",
       " '알려줘': 5,\n",
       " '명언': 6,\n",
       " '이': 7,\n",
       " '누구': 8,\n",
       " '알려줘요': 9,\n",
       " '날씨': 10,\n",
       " '주': 11,\n",
       " '지금': 12,\n",
       " '해줘': 13,\n",
       " '시간': 14,\n",
       " '뭐': 15,\n",
       " '노래': 16,\n",
       " '추천': 17,\n",
       " '상태': 18,\n",
       " '들려줘': 19,\n",
       " '몇': 20,\n",
       " '말': 21,\n",
       " '공기': 22,\n",
       " '이슈': 23,\n",
       " '요': 24,\n",
       " '무슨': 25,\n",
       " '좀': 26,\n",
       " '이번': 27,\n",
       " '맛집': 28,\n",
       " '요즘': 29,\n",
       " '나': 30,\n",
       " '알려줄래': 31,\n",
       " '주변': 32,\n",
       " '내일': 33,\n",
       " '해줘요': 34,\n",
       " '알려주라': 35,\n",
       " '지': 36,\n",
       " '며칠': 37,\n",
       " '알려줄래요': 38,\n",
       " '의': 39,\n",
       " '좋은': 40,\n",
       " '뉴스': 41,\n",
       " '사건': 42,\n",
       " '현재': 43,\n",
       " '이제': 44,\n",
       " '곧': 45,\n",
       " '번역': 46,\n",
       " '날짜': 47,\n",
       " '에요': 48,\n",
       " '에게': 49,\n",
       " '되는': 50,\n",
       " '인가요': 51,\n",
       " '뭔': 52,\n",
       " '거': 53,\n",
       " '핫': 54,\n",
       " '미세먼지': 55,\n",
       " '하나': 56,\n",
       " '시야': 57,\n",
       " '니': 58,\n",
       " '가장': 59,\n",
       " '화제': 60,\n",
       " '어디': 61,\n",
       " '중국어': 62,\n",
       " '있나요': 63,\n",
       " '이야': 64,\n",
       " '요일': 65,\n",
       " '인지': 66,\n",
       " '힘': 67,\n",
       " '들려줘요': 68,\n",
       " '일': 69,\n",
       " '에': 70,\n",
       " '마스크': 71,\n",
       " '어떻게': 72,\n",
       " '우산': 73,\n",
       " '해주라': 74,\n",
       " '가': 75,\n",
       " '해주세요': 76,\n",
       " '멋진': 77,\n",
       " '결과': 78,\n",
       " '바람': 79,\n",
       " '프랑스어': 80,\n",
       " '다음': 81,\n",
       " '있니': 82,\n",
       " '전': 83,\n",
       " '누군지': 84,\n",
       " '알려주세요': 85,\n",
       " '라': 86,\n",
       " '영어': 87,\n",
       " '음악': 88,\n",
       " '시': 89,\n",
       " '치킨': 90,\n",
       " '최근': 91,\n",
       " '분': 92,\n",
       " '수': 93,\n",
       " '해줄래': 94,\n",
       " '해줄래요': 95,\n",
       " '만': 96,\n",
       " '어때': 97,\n",
       " '비': 98,\n",
       " '음식': 99,\n",
       " '들려주라': 100,\n",
       " '야': 101,\n",
       " '아니': 102,\n",
       " '음식점': 103,\n",
       " '해': 104,\n",
       " '한': 105,\n",
       " '게': 106,\n",
       " '궁금해': 107,\n",
       " '해주': 108,\n",
       " '어때요': 109,\n",
       " '맛있는': 110,\n",
       " '알려줄': 111,\n",
       " '일본어': 112,\n",
       " '모레': 113,\n",
       " '있던': 114,\n",
       " '했던': 115,\n",
       " '유행': 116,\n",
       " '하는게': 117,\n",
       " '뜨는게': 118,\n",
       " '일이': 119,\n",
       " '어떤': 120,\n",
       " '부산': 121,\n",
       " '주요': 122,\n",
       " '많이': 123,\n",
       " '적': 124,\n",
       " '들려주세요': 125,\n",
       " '한테': 126,\n",
       " '죠': 127,\n",
       " '대구': 128,\n",
       " '되니': 129,\n",
       " '는': 130,\n",
       " '써야': 131,\n",
       " '눈': 132,\n",
       " '부니': 133,\n",
       " '들을래': 134,\n",
       " '전주': 135,\n",
       " '그': 136,\n",
       " '좋니': 137,\n",
       " '스페인어': 138,\n",
       " '신지': 139,\n",
       " '날': 140,\n",
       " '어떠니': 141,\n",
       " '12일': 142,\n",
       " '인': 143,\n",
       " '알려줄수': 144,\n",
       " '듣고': 145,\n",
       " '로': 146,\n",
       " '집': 147,\n",
       " '신문': 148,\n",
       " '어제': 149,\n",
       " '해봐': 150,\n",
       " '대전': 151,\n",
       " '서울': 152,\n",
       " '제': 153,\n",
       " '야구': 154,\n",
       " '날인': 155,\n",
       " '시지': 156,\n",
       " '오나요': 157,\n",
       " '아산': 158,\n",
       " '틀어줘': 159,\n",
       " '들려줄래': 160,\n",
       " '행복한': 161,\n",
       " '최신': 162,\n",
       " '단어': 163,\n",
       " '이지': 164,\n",
       " '이에요': 165,\n",
       " '먼지': 166,\n",
       " '있어': 167,\n",
       " '울산': 168,\n",
       " '제일': 169,\n",
       " '라틴어': 170,\n",
       " '기사': 171,\n",
       " '도': 172,\n",
       " '광명': 173,\n",
       " '들을래요': 174,\n",
       " '나온': 175,\n",
       " '얘기': 176,\n",
       " '조언': 177,\n",
       " '있어요': 178,\n",
       " '궁금해요': 179,\n",
       " '사회': 180,\n",
       " '했던것': 181,\n",
       " '뜨는': 182,\n",
       " '됐던': 183,\n",
       " '먹을': 184,\n",
       " '에픽하이': 185,\n",
       " '에서': 186,\n",
       " '정치': 187,\n",
       " '저번': 188,\n",
       " '아시나요': 189,\n",
       " '하늘': 190,\n",
       " '맑니': 191,\n",
       " '윤하': 192,\n",
       " '들려주겠나요': 193,\n",
       " '저': 194,\n",
       " '일까': 195,\n",
       " '다음주': 196,\n",
       " '누굴까': 197,\n",
       " '요슴': 198,\n",
       " '맛': 199,\n",
       " '아': 200,\n",
       " '알아요': 201,\n",
       " '농구': 202,\n",
       " '할까': 203,\n",
       " '하니': 204,\n",
       " '를': 205,\n",
       " '알려': 206,\n",
       " '모르겠어': 207,\n",
       " '4월': 208,\n",
       " '토요일': 209,\n",
       " '난': 210,\n",
       " '신나는': 211,\n",
       " '알': 212,\n",
       " '이탈리아어': 213,\n",
       " '니요': 214,\n",
       " '행복해지는': 215,\n",
       " '9월': 216,\n",
       " '인천': 217,\n",
       " '좋나요': 218,\n",
       " '오니': 219,\n",
       " '주제가': 220,\n",
       " '가요': 221,\n",
       " '문장': 222,\n",
       " '일본말': 223,\n",
       " '축구': 224,\n",
       " '이니': 225,\n",
       " '줘': 226,\n",
       " '맑은': 227,\n",
       " '경주': 228,\n",
       " '대통령': 229,\n",
       " '부나요': 230,\n",
       " '맛있니': 231,\n",
       " '은': 232,\n",
       " '재생': 233,\n",
       " '래': 234,\n",
       " '데': 235,\n",
       " '올림픽': 236,\n",
       " '있냐': 237,\n",
       " '좋아': 238,\n",
       " '어떤지': 239,\n",
       " '논산': 240,\n",
       " '3월': 241,\n",
       " '15일': 242,\n",
       " '포항': 243,\n",
       " '써야하나요': 244,\n",
       " '광주': 245,\n",
       " '구름': 246,\n",
       " '마포구': 247,\n",
       " '고기': 248,\n",
       " '피자': 249,\n",
       " '천안': 250,\n",
       " '들': 251,\n",
       " '정준영': 252,\n",
       " '합니다': 253,\n",
       " '단독': 254,\n",
       " '였는지': 255,\n",
       " '쓰는': 256,\n",
       " '달': 257,\n",
       " '돼요': 258,\n",
       " '해줄수': 259,\n",
       " '라고': 260,\n",
       " '누군데': 261,\n",
       " '있었니': 262,\n",
       " '있었나요': 263,\n",
       " '맑은가요': 264,\n",
       " '맛있어': 265,\n",
       " '먹을지': 266,\n",
       " '들려주겠니': 267,\n",
       " '좋던데': 268,\n",
       " '싶네': 269,\n",
       " '미국': 270,\n",
       " '사랑': 271,\n",
       " '독일어': 272,\n",
       " '그리스어': 273,\n",
       " '심심하거든': 274,\n",
       " '해줄': 275,\n",
       " '너': 276,\n",
       " '들려줘야': 277,\n",
       " '저기': 278,\n",
       " '혹시': 279,\n",
       " '번만': 280,\n",
       " '부탁': 281,\n",
       " '어떨까': 282,\n",
       " '쓸까': 283,\n",
       " '구로구': 284,\n",
       " '군산': 285,\n",
       " '오는지': 286,\n",
       " '우비': 287,\n",
       " '입을까': 288,\n",
       " '충주': 289,\n",
       " '있죠': 290,\n",
       " '뽀로로': 291,\n",
       " '와이': 292,\n",
       " '틀어주라': 293,\n",
       " '슬픈': 294,\n",
       " '싶어요': 295,\n",
       " 'ost': 296,\n",
       " '틀어': 297,\n",
       " '줬으면': 298,\n",
       " '오': 299,\n",
       " '차트': 300,\n",
       " '밤': 301,\n",
       " '해요': 302,\n",
       " '제라드': 303,\n",
       " '까먹었어': 304,\n",
       " '속초': 305,\n",
       " '있겠니': 306,\n",
       " '좋아요': 307,\n",
       " '제주': 308,\n",
       " '냐': 309,\n",
       " '와': 310,\n",
       " '맑나요': 311,\n",
       " '꼈나요': 312,\n",
       " '올까': 313,\n",
       " '흐리니': 314,\n",
       " '돼': 315,\n",
       " '들려줄수': 316,\n",
       " '목소리': 317,\n",
       " '기쁜': 318,\n",
       " '힙합': 319,\n",
       " '오빠': 320,\n",
       " '방탄': 321,\n",
       " '인기': 322,\n",
       " '줄': 323,\n",
       " '들려줄래요': 324,\n",
       " '냉장고': 325,\n",
       " '학교': 326,\n",
       " '사람': 327,\n",
       " '양양': 328,\n",
       " '보여': 329,\n",
       " '쓰는게': 330,\n",
       " '오산': 331,\n",
       " '있을까요': 332,\n",
       " '줘요': 333,\n",
       " '박': 334,\n",
       " '정몽주': 335,\n",
       " '진천': 336,\n",
       " '산천': 337,\n",
       " '동두천': 338,\n",
       " '김치': 339,\n",
       " '라면': 340,\n",
       " '떡갈비': 341,\n",
       " '갈비': 342,\n",
       " '언': 343,\n",
       " '나플': 344,\n",
       " '듣고싶어': 345,\n",
       " '가슴': 346,\n",
       " '고등': 347,\n",
       " '래퍼': 348,\n",
       " '수란': 349,\n",
       " '좋더라': 350,\n",
       " '을': 351,\n",
       " '가사': 352,\n",
       " '즐거운': 353,\n",
       " '에이핑크': 354,\n",
       " '주세요': 355,\n",
       " '우': 356,\n",
       " '존경': 357,\n",
       " '바보': 358,\n",
       " '누나': 359,\n",
       " '토마토': 360,\n",
       " '감': 361,\n",
       " '바지': 362,\n",
       " '고장': 363,\n",
       " '가방': 364,\n",
       " '밥': 365,\n",
       " '일어': 366,\n",
       " '아랍어': 367,\n",
       " '러시아어': 368,\n",
       " '소식': 369,\n",
       " '나왔니': 370,\n",
       " '읽어줘': 371,\n",
       " '읽어줄래요': 372,\n",
       " '태권도': 373,\n",
       " '펜싱': 374,\n",
       " '손흥민': 375,\n",
       " '읽어줘요': 376,\n",
       " '재미': 377,\n",
       " '있는': 378,\n",
       " '해주겠니': 379,\n",
       " '날이니요': 380,\n",
       " '알고있나': 381,\n",
       " '였나': 382,\n",
       " '강릉': 383,\n",
       " '안산': 384,\n",
       " '영등포': 385,\n",
       " '보면': 386,\n",
       " '써야하니': 387,\n",
       " '글피': 388,\n",
       " '것': 389,\n",
       " '금천구': 390,\n",
       " '소크라테스': 391,\n",
       " '미래': 392,\n",
       " '김수경': 393,\n",
       " '유재석': 394,\n",
       " '해시': 395,\n",
       " '스완': 396,\n",
       " '베': 397,\n",
       " '이즈': 398,\n",
       " '한국': 399,\n",
       " 'UN': 400,\n",
       " '사무': 401,\n",
       " '총장': 402,\n",
       " '대체': 403,\n",
       " '알고싶어': 404,\n",
       " '챙길까': 405,\n",
       " '홍대': 406,\n",
       " '이런': 407,\n",
       " '먹을까요': 408,\n",
       " '정': 409,\n",
       " '맛있죠': 410,\n",
       " '송천동': 411,\n",
       " '익산': 412,\n",
       " '제주도': 413,\n",
       " '디지몬': 414,\n",
       " '포켓몬': 415,\n",
       " '서태지': 416,\n",
       " '걸스데이': 417,\n",
       " '루피': 418,\n",
       " '아이오': 419,\n",
       " '아이': 420,\n",
       " '노을': 421,\n",
       " '3': 422,\n",
       " '길': 423,\n",
       " '다시': 424,\n",
       " '밴드': 425,\n",
       " '장범준': 426,\n",
       " '영화': 427,\n",
       " '기억': 428,\n",
       " '빅뱅': 429,\n",
       " '그레이': 430,\n",
       " '슬프더라': 431,\n",
       " '하': 432,\n",
       " '비투비': 433,\n",
       " '행주': 434,\n",
       " '트로트': 435,\n",
       " '듣고싶네': 436,\n",
       " '온': 437,\n",
       " '영': 438,\n",
       " '들려': 439,\n",
       " '아이돌': 440,\n",
       " '양': 441,\n",
       " '요요': 442,\n",
       " '목성': 443,\n",
       " '죽을래': 444,\n",
       " '창문': 445,\n",
       " '상어': 446,\n",
       " '할머니': 447,\n",
       " '났어': 448,\n",
       " '귀하': 449,\n",
       " '코뿔소': 450,\n",
       " '당신': 451,\n",
       " '얼마': 452,\n",
       " '해왕성': 453,\n",
       " '정성': 454,\n",
       " '이다': 455,\n",
       " '밥솥': 456,\n",
       " '고치': 457,\n",
       " '칠판': 458,\n",
       " '미안해': 459,\n",
       " '오징어': 460,\n",
       " '게임': 461,\n",
       " '초밥': 462,\n",
       " '티비': 463,\n",
       " '불법': 464,\n",
       " '읽어': 465,\n",
       " '즐': 466,\n",
       " '연예': 467,\n",
       " '이건희': 468,\n",
       " '아시안': 469,\n",
       " '현대': 470,\n",
       " '조던': 471,\n",
       " '승리': 472,\n",
       " '일있니요': 473,\n",
       " '순위': 474,\n",
       " '박미선': 475,\n",
       " '더': 476,\n",
       " '인데': 477,\n",
       " '다다': 478,\n",
       " '음': 479,\n",
       " '보령': 480,\n",
       " '영월': 481,\n",
       " '쓰기': 482,\n",
       " '싫은데': 483,\n",
       " '있을까': 484,\n",
       " '있겠나요': 485,\n",
       " '좋을까요': 486,\n",
       " '실': 487,\n",
       " '흐릴까': 488,\n",
       " '흐려': 489,\n",
       " '꼈니': 490,\n",
       " '챙겨야': 491,\n",
       " '고민': 492,\n",
       " '순천': 493,\n",
       " '평택': 494,\n",
       " '부냐': 495,\n",
       " '흐리나요': 496,\n",
       " '흐려요': 497,\n",
       " '공주': 498,\n",
       " '고깃집': 499,\n",
       " '왕': 500,\n",
       " '성남': 501,\n",
       " '송파구': 502,\n",
       " '술집': 503,\n",
       " '떡볶이': 504,\n",
       " '여수': 505,\n",
       " '음료수': 506,\n",
       " '레인보우': 507,\n",
       " '조현영': 508,\n",
       " '가인': 509,\n",
       " '브라운': 510,\n",
       " '이드': 511,\n",
       " '걸스': 512,\n",
       " '달샤벳': 513,\n",
       " '도끼': 514,\n",
       " '김효': 515,\n",
       " '매드': 516,\n",
       " '클라': 517,\n",
       " '운': 518,\n",
       " '어반자카파': 519,\n",
       " '어반': 520,\n",
       " '자': 521,\n",
       " '카파': 522,\n",
       " '로보트': 523,\n",
       " '씨잼': 524,\n",
       " '베이식': 525,\n",
       " '슈퍼': 526,\n",
       " '면도': 527,\n",
       " '에듀': 528,\n",
       " '케이트': 529,\n",
       " '콜라보': 530,\n",
       " 'wu': 531,\n",
       " '투': 532,\n",
       " '원': 533,\n",
       " '지아': 534,\n",
       " 'mp': 535,\n",
       " '허각': 536,\n",
       " '윤도현': 537,\n",
       " '넬': 538,\n",
       " '아이비': 539,\n",
       " '들으려고': 540,\n",
       " '만난': 541,\n",
       " '세계': 542,\n",
       " '우울한데': 543,\n",
       " '들려줄': 544,\n",
       " '애': 545,\n",
       " '드시어': 546,\n",
       " '런': 547,\n",
       " '쉐이프': 548,\n",
       " '오브': 549,\n",
       " '미': 550,\n",
       " '혁오': 551,\n",
       " '감성': 552,\n",
       " '아픈': 553,\n",
       " '발라드': 554,\n",
       " '잭스': 555,\n",
       " '키스': 556,\n",
       " '아프지': 557,\n",
       " '마': 558,\n",
       " 'EDM': 559,\n",
       " '안나는데': 560,\n",
       " '이름': 561,\n",
       " '모르겠는데': 562,\n",
       " '마룬': 563,\n",
       " '파이브': 564,\n",
       " '트둥': 565,\n",
       " '다비치': 566,\n",
       " '원더걸스': 567,\n",
       " '추억': 568,\n",
       " '미스': 569,\n",
       " '에이': 570,\n",
       " '수지': 571,\n",
       " '포미닛': 572,\n",
       " '뱅뱅뱅': 573,\n",
       " '히트': 574,\n",
       " '곡': 575,\n",
       " '하기나': 576,\n",
       " '크러쉬': 577,\n",
       " '블루스': 578,\n",
       " '재즈': 579,\n",
       " '증인': 580,\n",
       " '클럽': 581,\n",
       " '나온다면': 582,\n",
       " '우디': 583,\n",
       " '윤종신': 584,\n",
       " '오아시스': 585,\n",
       " '김범수': 586,\n",
       " '좋겠네': 587,\n",
       " '샤이니': 588,\n",
       " '청': 589,\n",
       " 'winner': 590,\n",
       " '싶어져': 591,\n",
       " '현아': 592,\n",
       " '베베': 593,\n",
       " '취하': 594,\n",
       " '면': 595,\n",
       " '지코': 596,\n",
       " '동방신기': 597,\n",
       " '주문': 598,\n",
       " '미로틱': 599,\n",
       " '마이걸': 600,\n",
       " '보이비': 601,\n",
       " '이센스': 602,\n",
       " '구수한': 603,\n",
       " '틀어줄래': 604,\n",
       " '알앤비': 605,\n",
       " '소울': 606,\n",
       " '첸': 607,\n",
       " 'make': 608,\n",
       " 'it': 609,\n",
       " 'count': 610,\n",
       " '멜론': 611,\n",
       " '숀': 612,\n",
       " 'way': 613,\n",
       " 'back': 614,\n",
       " 'home': 615,\n",
       " 'ariana': 616,\n",
       " 'grande': 617,\n",
       " 'aoa': 618,\n",
       " '소년단': 619,\n",
       " '싶어': 620,\n",
       " '때': 621,\n",
       " '김하': 622,\n",
       " '인기가요': 623,\n",
       " '우울한': 624,\n",
       " '희망': 625,\n",
       " '비극': 626,\n",
       " '뜨거운': 627,\n",
       " '싶은데': 628,\n",
       " '주겠니': 629,\n",
       " '그룹': 630,\n",
       " '랩': 631,\n",
       " '창모': 632,\n",
       " '한요': 633,\n",
       " '기리보이': 634,\n",
       " '홍진영': 635,\n",
       " '틀어줄래요': 636,\n",
       " '틀어줘요': 637,\n",
       " '오션': 638,\n",
       " '다': 639,\n",
       " '원재': 640,\n",
       " '태양': 641,\n",
       " '좋': 642,\n",
       " '더세요': 643,\n",
       " '소설': 644,\n",
       " '문어': 645,\n",
       " '코끼리': 646,\n",
       " '바라볼래요': 647,\n",
       " '텅': 648,\n",
       " '비었군요': 649,\n",
       " '파이프': 650,\n",
       " '탱크': 651,\n",
       " '에어컨': 652,\n",
       " '칫솔': 653,\n",
       " '마우스': 654,\n",
       " '가나': 655,\n",
       " '설거지': 656,\n",
       " '해야': 657,\n",
       " '깨끗하군요': 658,\n",
       " '들어요': 659,\n",
       " '사나이': 660,\n",
       " '돼지': 661,\n",
       " '배': 662,\n",
       " '여자': 663,\n",
       " '바이올린': 664,\n",
       " '셔츠': 665,\n",
       " '간장': 666,\n",
       " '공기청정기': 667,\n",
       " '퇴장': 668,\n",
       " '모니터': 669,\n",
       " '군인': 670,\n",
       " '먹고싶어요': 671,\n",
       " '속보': 672,\n",
       " '나왔었니': 673,\n",
       " '살인': 674,\n",
       " '아이티': 675,\n",
       " '홈런': 676,\n",
       " '컵': 677,\n",
       " '엘에이': 678,\n",
       " '다저스': 679,\n",
       " '삼성': 680,\n",
       " '에스케이': 681,\n",
       " '금호': 682,\n",
       " '타이어': 683,\n",
       " '케이씨씨': 684,\n",
       " '유나이티드': 685,\n",
       " '맨체스터': 686,\n",
       " '플레이오프': 687,\n",
       " '감자': 688,\n",
       " '김지수': 689,\n",
       " '무당': 690,\n",
       " '눈사람': 691,\n",
       " '박지성': 692,\n",
       " '궁전': 693,\n",
       " '문자': 694,\n",
       " '정수': 695,\n",
       " '오미연': 696,\n",
       " '장사꾼': 697,\n",
       " '소리': 698,\n",
       " '꾼': 699,\n",
       " '암탉': 700,\n",
       " '수탉': 701,\n",
       " '국회': 702,\n",
       " '있니요': 703,\n",
       " '북한': 704,\n",
       " '안보리': 705,\n",
       " '정상': 706,\n",
       " '회담': 707,\n",
       " '있었니요': 708,\n",
       " '가두리': 709,\n",
       " '특보': 710,\n",
       " '특종': 711,\n",
       " '컴퓨터': 712,\n",
       " '코파': 713,\n",
       " '해주겠나요': 714,\n",
       " '날이죠': 715,\n",
       " '모르겠네': 716,\n",
       " '였지': 717,\n",
       " '알고싶어요': 718,\n",
       " '였더라': 719,\n",
       " '되더라': 720,\n",
       " '일일': 721,\n",
       " '까': 722,\n",
       " '어떤데': 723,\n",
       " '완산구': 724,\n",
       " '좋은가요': 725,\n",
       " '고령': 726,\n",
       " '나주': 727,\n",
       " '부안': 728,\n",
       " '서천': 729,\n",
       " '영덕': 730,\n",
       " '가로수길': 731,\n",
       " '권지용': 732,\n",
       " '김지원': 733,\n",
       " '백': 734,\n",
       " '김동주': 735,\n",
       " '박근혜': 736,\n",
       " '이병철': 737,\n",
       " '최': 738,\n",
       " '순': 739,\n",
       " '시라소니': 740,\n",
       " '알고있니': 741,\n",
       " '유병재': 742,\n",
       " '신': 743,\n",
       " '고요': 744,\n",
       " '알고있죠': 745,\n",
       " '문': 746,\n",
       " '세종': 747,\n",
       " '맑아': 748,\n",
       " '청주': 749,\n",
       " '춘천': 750,\n",
       " '동구': 751,\n",
       " '옷': 752,\n",
       " '입어야': 753,\n",
       " '서산': 754,\n",
       " '부는지': 755,\n",
       " '송도': 756,\n",
       " '이천': 757,\n",
       " '상주': 758,\n",
       " '오냐': 759,\n",
       " '부천': 760,\n",
       " '일산': 761,\n",
       " '있지': 762,\n",
       " '거기': 763,\n",
       " '있지요': 764,\n",
       " '먹을까': 765,\n",
       " '어떤거': 766,\n",
       " '먹을만': 767,\n",
       " '알려주세': 768,\n",
       " '어떤게': 769,\n",
       " '소문난': 770,\n",
       " '유성구': 771,\n",
       " '비빔밥': 772,\n",
       " '라멘': 773,\n",
       " '수원': 774,\n",
       " '유명한': 775,\n",
       " '김밥': 776,\n",
       " '아이스크림': 777,\n",
       " '빵집': 778,\n",
       " '순창': 779,\n",
       " '양주': 780,\n",
       " '닭꼬치': 781,\n",
       " '새우': 782,\n",
       " '튀김': 783,\n",
       " '수제': 784,\n",
       " '버거': 785,\n",
       " '쌀국수': 786,\n",
       " '맛있어요': 787,\n",
       " '스윙스': 788,\n",
       " '빅스': 789,\n",
       " '더콰이엇': 790,\n",
       " '플로우': 791,\n",
       " '보아': 792,\n",
       " 'Boa': 793,\n",
       " 'bgm': 794,\n",
       " '엑소': 795,\n",
       " '애넥도트': 796,\n",
       " '버벌진트': 797,\n",
       " '소녀시대': 798,\n",
       " '그렇게': 799,\n",
       " 'believer': 800,\n",
       " '알리': 801,\n",
       " '손승연': 802,\n",
       " 'emd': 803,\n",
       " '성시경': 804,\n",
       " '감미롭던데': 805,\n",
       " '버스커': 806,\n",
       " '효린': 807,\n",
       " '하하': 808,\n",
       " '빡센': 809,\n",
       " '외힙': 810,\n",
       " '살기': 811,\n",
       " '위해': 812,\n",
       " '서': 813,\n",
       " '베토벤': 814,\n",
       " '클래식': 815,\n",
       " '팝송': 816,\n",
       " '브리티쉬': 817,\n",
       " '들을려고': 818,\n",
       " '아플': 819,\n",
       " '들을만한': 820,\n",
       " '힘들': 821,\n",
       " '듣기': 822,\n",
       " '아이콘': 823,\n",
       " '했다': 824,\n",
       " '닐': 825,\n",
       " '지나오다': 826,\n",
       " '멜': 827,\n",
       " '로망스': 828,\n",
       " '별': 829,\n",
       " '빛나는': 830,\n",
       " '빈첸': 831,\n",
       " '하선': 832,\n",
       " '호': 833,\n",
       " '허클베리피': 834,\n",
       " '붕붕': 835,\n",
       " '바코드': 836,\n",
       " '기분': 837,\n",
       " '이은미': 838,\n",
       " '녹턴': 839,\n",
       " '동요': 840,\n",
       " '재밌는': 841,\n",
       " '들려줄수있니': 842,\n",
       " '잔잔한': 843,\n",
       " '뉴에이지': 844,\n",
       " '시원한': 845,\n",
       " '상쾌한': 846,\n",
       " '걷는': 847,\n",
       " '소나무': 848,\n",
       " '이수': 849,\n",
       " '들려줬으면': 850,\n",
       " '좋겠어': 851,\n",
       " '좋다': 852,\n",
       " '그루비': 853,\n",
       " '룸': 854,\n",
       " '프로': 855,\n",
       " '듀스': 856,\n",
       " '코드': 857,\n",
       " '쿤스트': 858,\n",
       " '자이언티': 859,\n",
       " '양화대교': 860,\n",
       " '베베미뇽': 861,\n",
       " '듣고싶은데': 862,\n",
       " '10': 863,\n",
       " 'cm': 864,\n",
       " '세븐': 865,\n",
       " '틴': 866,\n",
       " '왼': 867,\n",
       " '리쌍': 868,\n",
       " '아름다워': 869,\n",
       " '싶네요': 870,\n",
       " '라붐': 871,\n",
       " '노엘': 872,\n",
       " '헬리콥터': 873,\n",
       " '릴': 874,\n",
       " '보이': 875,\n",
       " '먼데이': 876,\n",
       " '키즈': 877,\n",
       " '가울': 878,\n",
       " '안부': 879,\n",
       " '백예린': 880,\n",
       " '신곡': 881,\n",
       " '나왔던데': 882,\n",
       " '엔플라잉': 883,\n",
       " '옥탑방': 884,\n",
       " '재키': 885,\n",
       " '검': 886,\n",
       " '최하': 887,\n",
       " '민': 888,\n",
       " '웨이브': 889,\n",
       " '송민호': 890,\n",
       " '아낙네': 891,\n",
       " '고백': 892,\n",
       " '들어줄래요': 893,\n",
       " '벤의': 894,\n",
       " '연애': 895,\n",
       " '중': 896,\n",
       " '선미': 897,\n",
       " '누': 898,\n",
       " '아르': 899,\n",
       " '호불호': 900,\n",
       " '시차': 901,\n",
       " 'exid': 902,\n",
       " '러블': 903,\n",
       " '리즈': 904,\n",
       " '여자친구': 905,\n",
       " '걸그룹': 906,\n",
       " '후예': 907,\n",
       " '줄래요': 908,\n",
       " '2': 909,\n",
       " 'ne': 910,\n",
       " '1': 911,\n",
       " '싶어져요': 912,\n",
       " '모모': 913,\n",
       " '랜드': 914,\n",
       " '애프터스쿨': 915,\n",
       " '우주소녀': 916,\n",
       " '아스트로': 917,\n",
       " '틀어주세': 918,\n",
       " '스나이퍼': 919,\n",
       " '로꼬': 920,\n",
       " '비스트': 921,\n",
       " '좋던데요': 922,\n",
       " '노라조': 923,\n",
       " '형': 924,\n",
       " '한동근': 925,\n",
       " '끝': 926,\n",
       " '써': 927,\n",
       " '보려해': 928,\n",
       " '박진영': 929,\n",
       " '허니': 930,\n",
       " '트와이스': 931,\n",
       " '이쁘다': 932,\n",
       " '방탄소년단': 933,\n",
       " '멋있어': 934,\n",
       " '소고기': 935,\n",
       " '할아버지': 936,\n",
       " '토끼': 937,\n",
       " '호전': 938,\n",
       " '장갑': 939,\n",
       " '기차': 940,\n",
       " '청소기': 941,\n",
       " '명치': 942,\n",
       " '로마': 943,\n",
       " '상장': 944,\n",
       " '책상': 945,\n",
       " '박사': 946,\n",
       " '상기': 947,\n",
       " '멸치': 948,\n",
       " '리모컨': 949,\n",
       " '장치': 950,\n",
       " '수기': 951,\n",
       " '경찰': 952,\n",
       " '훼방': 953,\n",
       " '영국': 954,\n",
       " '지구': 955,\n",
       " '가위': 956,\n",
       " '동전': 957,\n",
       " '등산': 958,\n",
       " '남자': 959,\n",
       " '사과': 960,\n",
       " '이불': 961,\n",
       " '해파리': 962,\n",
       " '동생': 963,\n",
       " '기타': 964,\n",
       " '자전거': 965,\n",
       " '토성': 966,\n",
       " '고마워': 967,\n",
       " '등장': 968,\n",
       " '방문': 969,\n",
       " '파인애플': 970,\n",
       " '전기': 971,\n",
       " '물고기': 972,\n",
       " '배고파요': 973,\n",
       " '배고파서': 974,\n",
       " '잠': 975,\n",
       " '안오네요': 976,\n",
       " '가고싶어요': 977,\n",
       " '시장': 978,\n",
       " '왔어요': 979,\n",
       " '입어': 980,\n",
       " '야해요': 981,\n",
       " '침대': 982,\n",
       " '누워': 983,\n",
       " '수납': 984,\n",
       " '장': 985,\n",
       " '진심': 986,\n",
       " '으로': 987,\n",
       " '서랍': 988,\n",
       " '의자': 989,\n",
       " '소방관': 990,\n",
       " '읽어줄래': 991,\n",
       " '나왔던': 992,\n",
       " '뭐라고': 993,\n",
       " '했니': 994,\n",
       " '일있니': 995,\n",
       " '오지헌': 996,\n",
       " '헌': 997,\n",
       " '박명수': 998,\n",
       " '추사': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id = {t:i for i,t in enumerate(df.intent.unique())}\n",
    "id_to_label = {i:t for i,t in enumerate(df.intent.unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '음악',\n",
       " 1: '번역',\n",
       " 2: '뉴스',\n",
       " 3: '명언',\n",
       " 4: '달력',\n",
       " 5: '먼지',\n",
       " 6: '인물',\n",
       " 7: '시간',\n",
       " 8: '이슈',\n",
       " 9: '날씨',\n",
       " 10: '맛집'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "def tokenize_and_filter(sentences, labels):\n",
    "  inputs, outputs = [], []\n",
    "  \n",
    "  for sentence, label in zip(sentences, labels):\n",
    "    # tokenize sentence\n",
    "    tokenized_sentence = tokenizer.encode_a_doc_to_list(sentence)\n",
    "    # check tokenized sentence max length\n",
    "    if len(tokenized_sentence) <= MAX_LENGTH:\n",
    "      inputs.append(tokenized_sentence)\n",
    "      outputs.append(label_to_id[label])\n",
    "  \n",
    "  # pad tokenized sentences\n",
    "  padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      inputs, maxlen=MAX_LENGTH, padding='post', \n",
    "      value = tokenizer.vocabulary_['_PAD_']) # value = 0\n",
    "  \n",
    "  return padded_inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs = tokenize_and_filter(df.question, df.intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded input :  [185  73  19   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0] label :  0 original input sentence :  ['에픽하이', '우산', '들려줘', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_']\n"
     ]
    }
   ],
   "source": [
    "print('encoded input : ', inputs[0], 'label : ', outputs[0], 'original input sentence : ', tokenizer.decode_from_list(inputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 7836\n",
    "\n",
    "# decoder inputs use the previous target as input\n",
    "# remove START_TOKEN from targets\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  30    6   26  100    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  29   54  115   23    9    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  77  176   34    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 149  202   78    9    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  27   11   10   97    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 443   80   21   76    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [1307   81   11   10    5    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [1108  261   24    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  45   14  111   93   82    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 221  293    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  30   49   67    7   50    6   56   96  108   86    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  45   20   57    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  10  307    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  54  105  148   31    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  33  128   10   85    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  23  120  106  167    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]], shape=(16, 40), dtype=int32) tf.Tensor([3 8 3 2 9 1 9 6 7 0 3 7 9 2 9 8], shape=(16,), dtype=int32)\n",
      "-----------------------------------------------\n",
      "(16, 40) (16,)\n"
     ]
    }
   ],
   "source": [
    "for x, y in dataset.take(1):\n",
    "    print(x, y)\n",
    "    print('-----------------------------------------------')\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(len(label_to_id.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    return tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokenizer.n_vocabs, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(label_to_id.values()), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
    "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "225/225 [==============================] - 8s 38ms/step - loss: 2.3807 - sparse_categorical_accuracy: 0.1242\n",
      "Epoch 2/10\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 2.2094 - sparse_categorical_accuracy: 0.3105\n",
      "Epoch 3/10\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 1.5460 - sparse_categorical_accuracy: 0.6672\n",
      "Epoch 4/10\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 1.0225 - sparse_categorical_accuracy: 0.8516\n",
      "Epoch 5/10\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.6726 - sparse_categorical_accuracy: 0.9346\n",
      "Epoch 6/10\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.4475 - sparse_categorical_accuracy: 0.9552\n",
      "Epoch 7/10\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.3115 - sparse_categorical_accuracy: 0.9710\n",
      "Epoch 8/10\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2221 - sparse_categorical_accuracy: 0.9797\n",
      "Epoch 9/10\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1662 - sparse_categorical_accuracy: 0.9850\n",
      "Epoch 10/10\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1274 - sparse_categorical_accuracy: 0.9894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f909452d7f0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_processing(sentences):\n",
    "    inputs = []\n",
    "    for sentence in sentences:\n",
    "        # tokenize sentence\n",
    "        tokenized_sentence = tokenizer.encode_a_doc_to_list(sentence)\n",
    "        # check tokenized sentence max length\n",
    "        if len(tokenized_sentence) <= MAX_LENGTH:\n",
    "            inputs.append(tokenized_sentence)\n",
    "        else:\n",
    "            print('입력이 너무 길어요.')\n",
    "    # pad tokenized sentences\n",
    "    padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    inputs, maxlen=MAX_LENGTH, padding='post', \n",
    "    value = tokenizer.vocabulary_['_PAD_']) # value = 0\n",
    "    return padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = question_processing(['서울 날씨 어때?', \n",
    "                                      '나는 전주 날씨 궁금함',\n",
    "                                      '안중근 의사는 누구야?',\n",
    "                                      '이순신 장군은 어떤 분이니?',\n",
    "                                      '명동 맛있는 음식점 있니?'\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.3913907e-02, 3.8175474e-04, 3.3587371e-03, 3.0939284e-06,\n",
       "        5.9731095e-04, 1.3948370e-02, 5.5055339e-06, 1.9805663e-07,\n",
       "        7.0491456e-07, 9.4541907e-01, 2.3714271e-03],\n",
       "       [1.2016560e-01, 5.9382775e-04, 5.4020653e-03, 5.3703949e-05,\n",
       "        3.0661416e-03, 3.0298822e-03, 4.8764928e-06, 2.7779643e-06,\n",
       "        4.4299776e-07, 8.6604422e-01, 1.6365508e-03],\n",
       "       [2.2527718e-03, 2.4434816e-02, 1.2365759e-01, 3.3111421e-03,\n",
       "        4.4613713e-03, 4.1415058e-03, 7.4164480e-01, 2.3803756e-02,\n",
       "        1.7944578e-02, 6.2991661e-04, 5.3717792e-02],\n",
       "       [6.9524162e-03, 1.1055868e-01, 2.4180527e-01, 1.3723835e-02,\n",
       "        2.3659691e-01, 2.6755594e-03, 5.1567040e-02, 2.8376016e-01,\n",
       "        4.7570989e-02, 5.3829397e-04, 4.2508934e-03],\n",
       "       [1.6948736e-01, 4.4723932e-02, 3.7512951e-02, 2.0705439e-02,\n",
       "        3.9333210e-04, 7.6337932e-03, 2.0331221e-02, 1.7625947e-03,\n",
       "        2.2104169e-04, 1.1006074e-02, 6.8622226e-01]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9  9  6  7 10]\n"
     ]
    }
   ],
   "source": [
    "prediction = np.argmax(model.predict(input_sentence), axis=1)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "날씨\n",
      "날씨\n",
      "인물\n",
      "시간\n",
      "맛집\n"
     ]
    }
   ],
   "source": [
    "for p in prediction:\n",
    "    print(id_to_label[p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model.save(\"saved_intent_model.h5\")\n",
    "with open('id_to_intent.pickle', 'wb') as handle:\n",
    "    pickle.dump(id_to_label, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 추가해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['안중근', '이순신', '세종대왕', '김광석', '아이유', '에미넴', '이건희', '고아라', '유재석', '한석희', '최민성']\n",
    "def question_generator(names):\n",
    "    question = []\n",
    "    for name in names:\n",
    "        s1 = name+'는 어떤 분이야?'\n",
    "        s2 = name+'은 어떤 사람이니?'\n",
    "        s3 = name+'이란 사람에 대해 궁금해'\n",
    "        question = question+[s1, s2, s3]\n",
    "    return question\n",
    "question = question_generator(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안중근는 어떤 분이야?',\n",
       " '안중근은 어떤 사람이니?',\n",
       " '안중근이란 사람에 대해 궁금해',\n",
       " '이순신는 어떤 분이야?',\n",
       " '이순신은 어떤 사람이니?',\n",
       " '이순신이란 사람에 대해 궁금해',\n",
       " '세종대왕는 어떤 분이야?',\n",
       " '세종대왕은 어떤 사람이니?',\n",
       " '세종대왕이란 사람에 대해 궁금해',\n",
       " '김광석는 어떤 분이야?',\n",
       " '김광석은 어떤 사람이니?',\n",
       " '김광석이란 사람에 대해 궁금해',\n",
       " '아이유는 어떤 분이야?',\n",
       " '아이유은 어떤 사람이니?',\n",
       " '아이유이란 사람에 대해 궁금해',\n",
       " '에미넴는 어떤 분이야?',\n",
       " '에미넴은 어떤 사람이니?',\n",
       " '에미넴이란 사람에 대해 궁금해',\n",
       " '이건희는 어떤 분이야?',\n",
       " '이건희은 어떤 사람이니?',\n",
       " '이건희이란 사람에 대해 궁금해',\n",
       " '고아라는 어떤 분이야?',\n",
       " '고아라은 어떤 사람이니?',\n",
       " '고아라이란 사람에 대해 궁금해',\n",
       " '유재석는 어떤 분이야?',\n",
       " '유재석은 어떤 사람이니?',\n",
       " '유재석이란 사람에 대해 궁금해',\n",
       " '한석희는 어떤 분이야?',\n",
       " '한석희은 어떤 사람이니?',\n",
       " '한석희이란 사람에 대해 궁금해',\n",
       " '최민성는 어떤 분이야?',\n",
       " '최민성은 어떤 사람이니?',\n",
       " '최민성이란 사람에 대해 궁금해']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {'question' : question, 'intent' : ['인물']*len(question)}\n",
    "add_df = pd.DataFrame(new_data, columns=('question', 'intent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>안중근는 어떤 분이야?</td>\n",
       "      <td>인물</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>안중근은 어떤 사람이니?</td>\n",
       "      <td>인물</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>안중근이란 사람에 대해 궁금해</td>\n",
       "      <td>인물</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이순신는 어떤 분이야?</td>\n",
       "      <td>인물</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>이순신은 어떤 사람이니?</td>\n",
       "      <td>인물</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           question intent\n",
       "0      안중근는 어떤 분이야?     인물\n",
       "1     안중근은 어떤 사람이니?     인물\n",
       "2  안중근이란 사람에 대해 궁금해     인물\n",
       "3      이순신는 어떤 분이야?     인물\n",
       "4     이순신은 어떤 사람이니?     인물"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3591 33\n"
     ]
    }
   ],
   "source": [
    "print(len(df), len(add_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3624\n"
     ]
    }
   ],
   "source": [
    "new_df = pd.concat([df, add_df])\n",
    "print(len(new_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning was done                                        \n",
      "1413 terms are recognized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<vectorizer.BaseVectorizer at 0x7f914c3b0f98>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.fit(new_df['question'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inputs, new_outputs = tokenize_and_filter(new_df.question, new_df.intent)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 7836\n",
    "\n",
    "# decoder inputs use the previous target as input\n",
    "# remove START_TOKEN from targets\n",
    "dataset = tf.data.Dataset.from_tensor_slices((new_inputs, new_outputs))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "227/227 [==============================] - 9s 39ms/step - loss: 2.3841 - sparse_categorical_accuracy: 0.2208\n",
      "Epoch 2/10\n",
      "227/227 [==============================] - 6s 28ms/step - loss: 2.2262 - sparse_categorical_accuracy: 0.3629\n",
      "Epoch 3/10\n",
      "227/227 [==============================] - 6s 28ms/step - loss: 1.5886 - sparse_categorical_accuracy: 0.6388\n",
      "Epoch 4/10\n",
      "227/227 [==============================] - 6s 27ms/step - loss: 1.0611 - sparse_categorical_accuracy: 0.8295\n",
      "Epoch 5/10\n",
      "227/227 [==============================] - 6s 27ms/step - loss: 0.7302 - sparse_categorical_accuracy: 0.9180\n",
      "Epoch 6/10\n",
      "227/227 [==============================] - 6s 27ms/step - loss: 0.4943 - sparse_categorical_accuracy: 0.9473\n",
      "Epoch 7/10\n",
      "227/227 [==============================] - 6s 27ms/step - loss: 0.3430 - sparse_categorical_accuracy: 0.9614\n",
      "Epoch 8/10\n",
      "227/227 [==============================] - 6s 27ms/step - loss: 0.2467 - sparse_categorical_accuracy: 0.9708\n",
      "Epoch 9/10\n",
      "227/227 [==============================] - 6s 28ms/step - loss: 0.1842 - sparse_categorical_accuracy: 0.9785\n",
      "Epoch 10/10\n",
      "227/227 [==============================] - 6s 27ms/step - loss: 0.1426 - sparse_categorical_accuracy: 0.9829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9008175eb8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = get_model()\n",
    "LEARNING_RATE = 0.0001\n",
    "new_model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
    "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "new_model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = question_processing(['서울 날씨 어때?', \n",
    "                                      '나는 전주 날씨 궁금함',\n",
    "                                      '안중근 의사는 누구야?',\n",
    "                                      '박소희는 어떤 사람인지 궁금해.',\n",
    "                                      '명동 맛있는 음식점 있니?'\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.9589213e-02, 1.2809281e-02, 2.7919335e-03, 3.8396262e-05,\n",
       "        1.6206429e-04, 2.8144280e-04, 7.2826454e-03, 3.6325629e-03,\n",
       "        5.7560985e-04, 9.2276138e-01, 7.5480042e-05],\n",
       "       [7.8573801e-02, 1.5017393e-02, 1.3839473e-03, 9.5085154e-05,\n",
       "        4.4474233e-04, 1.7480650e-04, 2.0139690e-02, 1.9588403e-02,\n",
       "        4.4169917e-04, 8.6411631e-01, 2.4144043e-05],\n",
       "       [3.6772344e-02, 4.7681931e-02, 1.4293131e-03, 1.2719001e-02,\n",
       "        1.1210305e-02, 1.5252599e-05, 8.8609505e-01, 1.2578344e-03,\n",
       "        4.3560646e-04, 2.2382506e-03, 1.4516020e-04],\n",
       "       [4.6258370e-04, 1.0291640e-02, 3.7665705e-03, 9.9312933e-04,\n",
       "        5.1564139e-01, 1.6445718e-03, 4.5726588e-01, 7.9946313e-03,\n",
       "        1.6727225e-05, 1.8878463e-03, 3.5130801e-05],\n",
       "       [2.8394945e-02, 1.8452832e-02, 1.2928452e-01, 1.9538615e-02,\n",
       "        1.8960953e-05, 6.8824222e-05, 7.3304982e-03, 1.7405448e-06,\n",
       "        2.2738019e-01, 3.3468247e-04, 5.6919414e-01]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.predict(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(new_model.predict(input_sentence), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "날씨\n",
      "날씨\n",
      "인물\n",
      "달력\n",
      "맛집\n"
     ]
    }
   ],
   "source": [
    "for p in prediction:\n",
    "    print(id_to_label[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_juns)",
   "language": "python",
   "name": "conda_juns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
