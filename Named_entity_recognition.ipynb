{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER with Bidirectional-LSTM-CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original paper can be found at https://arxiv.org/abs/1511.08308"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # Colab only\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/juns/lib/python3.6/site-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n",
      "/home/ubuntu/anaconda3/envs/juns/lib/python3.6/site-packages/jpype/_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from konlpy.tag import Twitter\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from matplotlib import pyplot as plt\n",
    "from data_utils import createBatches, iterate_minibatches\n",
    "import ner\n",
    "keras = tf.keras\n",
    "t = Twitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. read train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Entity/train_entity.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, encoding='euc-kr') as f:\n",
    "    tokenized_sentences, labels = [], []\n",
    "    tokenized_sentence, label = [], []\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if (len(line) == 0 or line.startswith(\"-DOCSTART-\")):\n",
    "            if not len(tokenized_sentence) == 0:\n",
    "                tokenized_sentences.append(tokenized_sentence)\n",
    "                labels.append(label)\n",
    "                tokenized_sentence, label = [], [] #초기화\n",
    "        else:\n",
    "            word, tag = line.split(' ')\n",
    "            tokenized_sentence.append(word)\n",
    "            label.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['다음', '주', '전주', '비', '오려나'],\n",
       " ['이번', '주', '서울', '눈', '오니'],\n",
       " ['이번', '주', '대구', '날씨', '비', '오는지', '알려줘'],\n",
       " ['부산', '이번', '주', '날씨', '맑은지', '알려줄래'],\n",
       " ['광주', '이번', '주', '눈', '오려나'],\n",
       " ['울산', '이번', '주', '비', '오니'],\n",
       " ['이번', '주', '인천', '날씨', '눈', '오니'],\n",
       " ['이번', '주', '전주', '비', '오는지', '알려줄래'],\n",
       " ['이번', '주', '강릉', '맑아'],\n",
       " ['이번', '주', '속초', '날씨', '맑니'],\n",
       " ['이번', '주', '과천', '눈', '오니'],\n",
       " ['이번', '주', '군산', '비', '오니'],\n",
       " ['다음', '주', '전주', '날씨', '알려줘'],\n",
       " ['이번', '주', '서울', '날씨', '알려줘'],\n",
       " ['이번', '주', '대구', '날씨', '알려줘'],\n",
       " ['이번', '주', '부산', '날씨', '알려줘'],\n",
       " ['이번', '주', '광주', '날씨', '알려줘'],\n",
       " ['이번', '주', '울산', '날씨', '알려줘'],\n",
       " ['이번', '주', '인천', '날씨', '알려줘'],\n",
       " ['이번', '주', '전주', '날씨', '알려줘']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get vectorizer and fix some data error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorizer import BaseVectorizer\n",
    "vectorizer = BaseVectorizer(t.morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sentence :  ['모레', '송파구', '날씨', '알려줘'] \n",
      " labels :  ['DATE', 'LOCATION', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(\"tokenized sentence : \", tokenized_sentences[1606], '\\n',\n",
    "      \"labels : \", labels[1606])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['말', '해줄래']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.tokenizer('말해줄래') #우리 tokenizer와 다른 형태소 분석 형태로 데이터가 구성되어 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ts, label in zip(tokenized_sentences, labels):\n",
    "    for idx, word in enumerate(ts):\n",
    "        if len(vectorizer.tokenizer(word))>1:\n",
    "            tokenized_word = vectorizer.tokenizer(word)\n",
    "            ts.pop(idx)\n",
    "            tag = label.pop(idx)\n",
    "            for i in tokenized_word[::-1]:\n",
    "                ts.insert(idx, i)\n",
    "                label.insert(idx, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['모레', '송파구', '날씨', '알려줘']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences[1606]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DATE', 'LOCATION', 'O', 'O']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[1606]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create word vocabulary and char vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for ts in tokenized_sentences:\n",
    "    sentence = ' '.join(ts)\n",
    "    sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning was done                                        \n",
      "535 terms are recognized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<vectorizer.BaseVectorizer at 0x7f394403f668>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(sentences) #create word vocabulary from docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_char2idx() \n",
    "#create dictionary for converting char into index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_PAD_': 0, '_UNK_': 1, '_': 2, 'P': 3, 'A': 4, 'D': 5, 'U': 6, 'N': 7, 'K': 8, 'S': 9, 'T': 10, 'E': 11, 'O': 12, '날': 13, '씨': 14, '우': 15, '산': 16, '하': 17, '니': 18, '내': 19, '일': 20, '오': 21, '늘': 22, '써': 23, '야': 24, '주': 25, '알': 26, '려': 27, '줘': 28, '이': 29, '번': 30, '모': 31, '레': 32, '말': 33, '줄': 34, '래': 35, '마': 36, '스': 37, '크': 38, '비': 39, '챙': 40, '길': 41, '까': 42, '해': 43, '세': 44, '요': 45, '울': 46, '옷': 47, '입': 48, '을': 49, '어': 50, '상': 51, '태': 52, '전': 53, '서': 54, '쓸': 55, '부': 56, '광': 57, '떠': 58, '뭐': 59, '공': 60, '기': 61, '눈': 62, '북': 63, '지': 64, '좀': 65, '인': 66, '천': 67, '군': 68, '좋': 69, '쓰': 70, '고': 71, '민': 72, '다': 73, '음': 74, '돼': 75, '겨': 76, '때': 77, '나': 78, '뭔': 79, '맑': 80, '있': 81, '양': 82, '대': 83, '구': 84, '라': 85, '바': 86, '람': 87, '강': 88, '릉': 89, '경': 90, '도': 91, '원': 92, '포': 93, '싫': 94, '은': 95, '데': 96, '는': 97, '수': 98, '아': 99, '남': 100, '시': 101, '충': 102, '청': 103, '떤': 104, '미': 105, '먼': 106, '속': 107, '초': 108, '과': 109, '안': 110, '완': 111, '불': 112, '삼': 113, '례': 114, '동': 115, '신': 116, '촌': 117, '종': 118, '로': 119, '여': 120, '의': 121, '창': 122, '암': 123, '리': 124, '진': 125, '횡': 126, '성': 127, '척': 128, '령': 129, '제': 130, '많': 131, '평': 132, '택': 133, '중': 134, '무': 135, '1': 136, '2': 137, '월': 138, '금': 139, '룡': 140, '김': 141, '노': 142, '당': 143, '예': 144, '홍': 145, '정': 146, '짜': 147, '역': 148, '옥': 149, '면': 150, '장': 151, '순': 152, '9': 153, '관': 154, '악': 155, '두': 156, '흥': 157, '화': 158, '용': 159, '영': 160, '등': 161, '송': 162, '파': 163, '봉': 164, '단': 165, '임': 166, '실': 167, '항': 168, '문': 169, '사': 170, '녕': 171, '농': 172, '8': 173, '3': 174, '괴': 175, '되': 176, '네': 177, '달': 178, '춘': 179, '특': 180, '별': 181, '명': 182, '왕': 183, '작': 184, '백': 185, '선': 186, '보': 187, '증': 188, '읍': 189, '논': 190, '계': 191, '익': 192, '곡': 193, '함': 194, '목': 195, '담': 196, '위': 197, '덕': 198, '거': 199, '0': 200, '통': 201, '합': 202, '밀': 203, '한': 204, '림': 205, '애': 206, '좌': 207, '5': 208, '7': 209, '6': 210, '반': 211, '조': 212, '운': 213, '가': 214, '분': 215, '겠': 216, '철': 217, '칠': 218, '4': 219, '귀': 220, '표': 221, '류': 222, '황': 223, '연': 224, '희': 225, '검': 226, '효': 227, '앙': 228, '복': 229, '유': 230, '자': 231, '치': 232, '십': 233, '퍼': 234}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2idx = {}\n",
    "idx2label = []\n",
    "for label in labels:\n",
    "    for l in label:\n",
    "        if l not in label2idx:\n",
    "            label2idx[l] = len(label2idx)\n",
    "            idx2label.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DATE': 0, 'LOCATION': 1, 'O': 2} ['DATE', 'LOCATION', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(label2idx, idx2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Prepare dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 15\n",
    "\n",
    "def padding_char_indice(char_indice, MAX_LENGTH):\n",
    "    return tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      char_indice, maxlen=MAX_LENGTH, padding='post', \n",
    "      value = vectorizer.char2idx['_PAD_'])\n",
    "    \n",
    "\n",
    "def integer_coding(tokenized_sentences, labels):\n",
    "    dataset = []\n",
    "    for ts, label in zip(tokenized_sentences, labels):\n",
    "        word_indice = [vectorizer.vocabulary_[t] for t in ts]\n",
    "        char_indice = [[vectorizer.char2idx[char] for char in t]  \n",
    "                                                     for t in ts]\n",
    "        char_indice = padding_char_indice(char_indice, MAX_LENGTH)\n",
    "        label_indice = [label2idx[l] for l in label]\n",
    "    \n",
    "        for chars_of_token in char_indice:\n",
    "            if len(chars_of_token)>MAX_LENGTH:\n",
    "                print(\"최대 단어 길이 초과!\")\n",
    "                continue\n",
    "        dataset.append([word_indice, char_indice, label_indice])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = integer_coding(tokenized_sentences, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indice, char_indice, label_indice = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49, 10, 27, 17, 55]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['다음', '주', '전주', '비', '오려나']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.decode_from_list(word_indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 2, 2]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DATE', 'DATE', 'LOCATION', 'O', 'O']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idx2label[l] for l in label_indice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[73, 74,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [25,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [53, 25,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [39,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [21, 27, 78,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_indice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"./Entity\", exist_ok=True)\n",
    "np.save(\"Entity/idx2Label.npy\",idx2label)\n",
    "np.save(\"Entity/word2Idx.npy\",vectorizer.vocabulary_)\n",
    "np.save(\"Entity/char2Idx.npy\",vectorizer.char2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch,train_batch_len = createBatches(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 2)\n",
      "(637, 3)\n",
      "(1043, 4)\n",
      "(1509, 5)\n",
      "(399, 6)\n",
      "(157, 7)\n",
      "(14, 8)\n",
      "(4, 9)\n",
      "(2, 24)\n"
     ]
    }
   ],
   "source": [
    "for i,batch in enumerate(iterate_minibatches(train_batch,train_batch_len)):\n",
    "        labels, tokens ,char = batch\n",
    "        print(tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, TimeDistributed, Dropout, concatenate, Bidirectional, LSTM, Conv1D, Dense, MaxPooling1D, Flatten\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.utils import Progbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         [(None, None, 15)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (TimeDistributed (None, None, 15, 30) 7050        char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 15, 30) 0           char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 15, 30) 2730        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 1, 30)  0           time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "words_input (InputLayer)        [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, None, 30)     0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 64)     34240       words_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 30)     0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, 94)     0           embedding[0][0]                  \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, None, 400)    472000      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, None, 3)      1203        bidirectional[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 517,223\n",
      "Trainable params: 517,223\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "words_input = Input(shape=(None,),dtype='int32',name='words_input')\n",
    "words = Embedding(input_dim=len(vectorizer.vocabulary_), output_dim=64)(words_input)\n",
    "character_input=Input(shape=(None,MAX_LENGTH,),name='char_input')\n",
    "embed_char_out=TimeDistributed(Embedding(len(vectorizer.char2idx),30,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "dropout= Dropout(0.5)(embed_char_out)\n",
    "conv1d_out= TimeDistributed(Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1))(dropout)\n",
    "maxpool_out=TimeDistributed(MaxPooling1D(MAX_LENGTH))(conv1d_out)\n",
    "char = TimeDistributed(Flatten())(maxpool_out)\n",
    "char = Dropout(0.5)(char)\n",
    "output = concatenate([words, char])\n",
    "output = Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.25))(output)\n",
    "output = TimeDistributed(Dense(len(label2idx), activation='softmax'))(output)\n",
    "model = Model(inputs=[words_input,character_input], outputs=[output])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(model, 'model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/20\n",
      "9/9 [==============================] - 3s 295ms/step\n",
      " \n",
      "Epoch 1/20\n",
      "9/9 [==============================] - 2s 214ms/step\n",
      " \n",
      "Epoch 2/20\n",
      "9/9 [==============================] - 2s 219ms/step\n",
      " \n",
      "Epoch 3/20\n",
      "9/9 [==============================] - 2s 218ms/step\n",
      " \n",
      "Epoch 4/20\n",
      "9/9 [==============================] - 2s 225ms/step\n",
      " \n",
      "Epoch 5/20\n",
      "9/9 [==============================] - 2s 228ms/step\n",
      " \n",
      "Epoch 6/20\n",
      "9/9 [==============================] - 2s 223ms/step\n",
      " \n",
      "Epoch 7/20\n",
      "9/9 [==============================] - 2s 228ms/step\n",
      " \n",
      "Epoch 8/20\n",
      "9/9 [==============================] - 2s 218ms/step\n",
      " \n",
      "Epoch 9/20\n",
      "9/9 [==============================] - 2s 231ms/step\n",
      " \n",
      "Epoch 10/20\n",
      "9/9 [==============================] - 2s 211ms/step\n",
      " \n",
      "Epoch 11/20\n",
      "9/9 [==============================] - 2s 209ms/step\n",
      " \n",
      "Epoch 12/20\n",
      "9/9 [==============================] - 2s 226ms/step\n",
      " \n",
      "Epoch 13/20\n",
      "9/9 [==============================] - 2s 222ms/step\n",
      " \n",
      "Epoch 14/20\n",
      "9/9 [==============================] - 2s 212ms/step\n",
      " \n",
      "Epoch 15/20\n",
      "9/9 [==============================] - 2s 226ms/step\n",
      " \n",
      "Epoch 16/20\n",
      "4/9 [============>.................] - ETA: 1s"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):    \n",
    "    print(\"Epoch %d/%d\"%(epoch,epochs))\n",
    "    a = Progbar(len(train_batch_len))\n",
    "    for i,batch in enumerate(iterate_minibatches(train_batch,train_batch_len)):\n",
    "        labels, tokens, char = batch       \n",
    "        model.train_on_batch([tokens, char], labels)\n",
    "        a.update(i)\n",
    "    a.update(i+1)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Entity/model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_parser = ner.Parser(t.morphs)\n",
    "ner_parser.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('내일', 'DATE'), ('부산', 'LOCATION'), ('날씨', 'O'), ('는', 'O'), ('?', 'O')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_parser.predict('내일 부산 날씨는?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('오늘', 'DATE'), ('서울', 'LOCATION'), ('날씨', 'O'), ('어때', 'O'), ('?', 'O')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_parser.predict('오늘 서울 날씨 어때?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9월', 'DATE'),\n",
       " ('15일', 'DATE'),\n",
       " ('수유동', 'LOCATION'),\n",
       " ('날씨', 'O'),\n",
       " ('궁금해', 'O')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_parser.predict('9월 15일 수유동 날씨 궁금해')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_juns)",
   "language": "python",
   "name": "conda_juns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
